{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU\n",
      "GPUs Count: 1\n"
     ]
    }
   ],
   "source": [
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "from PIL import Image\n",
    "from torch.optim import SGD, lr_scheduler\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from fvcore.nn import FlopCountAnalysis\n",
    "\n",
    "from dataloaders.cifar10 import cifar10_dataloaders\n",
    "from dataloaders.cifar100 import cifar100_dataloaders\n",
    "from dataloaders.svhn import svhn_dataloaders\n",
    "from dataloaders.stl10 import stl10_dataloaders\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('GPU') if str(device) == \"cuda:0\" else print('GPU not Detected - CPU Selected')\n",
    "print(f\"GPUs Count: {torch.cuda.device_count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_train_batch_size = 128\n",
    "DATASET = 'mnist'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DATASET == 'cifar10':\n",
    "    train_loader, val_loader, test_loader = cifar10_dataloaders(train_batch_size=my_train_batch_size, num_workers=2)\n",
    "elif DATASET == 'cifar100':\n",
    "    train_loader, val_loader, test_loader = cifar100_dataloaders(train_batch_size=my_train_batch_size, num_workers=2)\n",
    "elif DATASET == 'svhn':\n",
    "    train_loader, val_loader, test_loader = svhn_dataloaders(train_batch_size=my_train_batch_size, num_workers=2)\n",
    "elif DATASET == 'stl10':\n",
    "    train_loader, val_loader = stl10_dataloaders(train_batch_size=my_train_batch_size, num_workers=2)\n",
    "elif DATASET == 'mnist':\n",
    "    transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "    train_dataset = Subset(datasets.MNIST('./data', train=True, transform=transform, download=True), list(range(50000)))\n",
    "    val_dataset = Subset(datasets.MNIST('./data', train=True, transform=transform, download=True), list(range(50000, 60000)))\n",
    "    test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=my_train_batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=100, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=100, shuffle=False)\n",
    "\n",
    "elif DATASET == 'emnist':\n",
    "    my_split = 'letters'  # options: 'byclass', 'bymerge', 'balanced', 'letters', 'digits', 'mnist'\n",
    "    transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "    train_dataset = datasets.EMNIST(root='./data', split=my_split, train=True, transform=transform, download=True)\n",
    "    test_dataset = datasets.EMNIST(root='./data', split=my_split, train=False, download=True, transform=transform)\n",
    "\n",
    "    train_dataset.targets -= 1    # Shift labels from 1-26 to 0-25\n",
    "    test_dataset.targets -= 1     # Shift labels from 1-26 to 0-25\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=my_train_batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=100, shuffle=False)\n",
    "\n",
    "elif DATASET == 'fashion_mnist':\n",
    "    transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.2860,), (0.3530,))])\n",
    "    train_dataset = Subset(datasets.FashionMNIST('./data', train=True, transform=transform, download=True), list(range(50000)))\n",
    "    val_dataset = Subset(datasets.FashionMNIST('./data', train=True, transform=transform, download=True), list(range(50000, 60000)))\n",
    "    test_dataset = datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=my_train_batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=100, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=100, shuffle=False)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(391, 100, 100)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader), len(val_loader), len(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DATASET == 'mnist' or DATASET == 'emnist' or DATASET == 'fashion_mnist':\n",
    "    image_channels = 1\n",
    "else:\n",
    "    image_channels = 3\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self, in_planes, mid_planes, out_planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, mid_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(mid_planes)\n",
    "        self.conv2 = nn.Conv2d(mid_planes, out_planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_planes)\n",
    "\n",
    "        if stride != 1 or in_planes != out_planes:\n",
    "            self.mismatch = True\n",
    "            self.conv_sc = nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
    "            self.bn_sc = nn.BatchNorm2d(out_planes)\n",
    "        else:\n",
    "            self.mismatch = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "\n",
    "        if self.mismatch:\n",
    "            out = out + self.bn_sc(self.conv_sc(x))\n",
    "        else:\n",
    "            out = out + x\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "class ResNet18(nn.Module):\n",
    "    def __init__(self, input_channels = image_channels, num_classes = 10):\n",
    "        super(ResNet18, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(input_channels, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        self.blocks = nn.ModuleList([\n",
    "            BasicBlock(64, 64, 64, stride=1),\n",
    "            BasicBlock(64, 64, 64, stride=1),\n",
    "\n",
    "            BasicBlock(64, 128, 128, stride=2),\n",
    "            BasicBlock(128, 128, 128, stride=1),\n",
    "\n",
    "            BasicBlock(128, 256, 256, stride=2),\n",
    "            BasicBlock(256, 256, 256, stride=1),\n",
    "\n",
    "            BasicBlock(256, 512, 512, stride=2),\n",
    "            BasicBlock(512, 512, 512, stride=1),\n",
    "            ])\n",
    "\n",
    "        self.linear_1 = nn.Linear(64, num_classes)\n",
    "        self.linear_2 = nn.Linear(64, num_classes)\n",
    "        self.linear_3 = nn.Linear(128, num_classes)\n",
    "        self.linear_4 = nn.Linear(128, num_classes)\n",
    "        self.linear_5 = nn.Linear(256, num_classes)\n",
    "        self.linear_6 = nn.Linear(256, num_classes)\n",
    "        self.linear_7 = nn.Linear(512, num_classes)\n",
    "        self.linear_8 = nn.Linear(512, num_classes)\n",
    "        \n",
    "    def forward(self, x, depth_level=8):      \n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        i = 0\n",
    "        for block in self.blocks:\n",
    "            i=i+1\n",
    "            out = block(out)\n",
    "            if i == depth_level:\n",
    "                break\n",
    "                        \n",
    "        out = F.adaptive_avg_pool2d(out, (1, 1))\n",
    "        out = out.view(out.size(0), -1)\n",
    "        \n",
    "        if depth_level == 1:\n",
    "            out = self.linear_1(out)\n",
    "        elif depth_level == 2:\n",
    "            out = self.linear_2(out)\n",
    "        elif depth_level == 3:\n",
    "            out = self.linear_3(out)\n",
    "        elif depth_level == 4:\n",
    "            out = self.linear_4(out)\n",
    "        elif depth_level == 5:\n",
    "            out = self.linear_5(out)\n",
    "        elif depth_level == 6:\n",
    "            out = self.linear_6(out)\n",
    "        elif depth_level == 7:\n",
    "            out = self.linear_7(out)\n",
    "        elif depth_level == 8:\n",
    "            out = self.linear_8(out)\n",
    "        return out\n",
    "\n",
    "model = ResNet18().to(device)\n",
    "max_depth = 8   # Number of blocks (total depth levels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Training Loop**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "depths = [1,2,3,4,5,6,7,8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################### Train Model Till Convergence\n",
    "def warm_up(model, epochs, learning_rate, depths):\n",
    "    optimizer = SGD(model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=5e-4)\n",
    "    scheduler = lr_scheduler.CosineAnnealingLR(optimizer, epochs)\n",
    "    for epoch in range(1, epochs+1):\n",
    "        model.train()\n",
    "        requires_grad_(model, True)\n",
    "        accs, losses = AverageMeter(), AverageMeter()\n",
    "        i = 0\n",
    "        for imgs, labels in train_loader:\n",
    "            i+=1\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            for depth_level in sorted(depths, reverse=True):\n",
    "                logits = model(imgs, depth_level)\n",
    "                loss = torch.mean(F.cross_entropy(logits, labels, reduction='none'))\n",
    "                accs.append((logits.argmax(1) == labels).float().mean().item() * 100)\n",
    "                losses.append(loss.item())\n",
    "                loss.backward()\n",
    "            optimizer.step()\n",
    "        train_str = f\"Epoch: {epoch} | Loss: {losses.avg:.4f} | Accuracy: {accs.avg:.2f} %\"\n",
    "        print(train_str)\n",
    "        scheduler.step()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "####################################### Train Model Till Convergence\n",
    "def train_till_convergence(model, target_accuracy, tolerance = 0.5, initial_depth = 1, max_depth = max_depth, learning_rate = 0.1):\n",
    "    depth_level = initial_depth\n",
    "    fine_tune_flag = 0\n",
    "    stopping_epochs = 23\n",
    "    no_improvement = 0\n",
    "    epoch_id = 0\n",
    "    optimizer = SGD(model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=5e-4)\n",
    "    scheduler = lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.6)\n",
    "\n",
    "    best_val_acc = 0.0\n",
    "    train_losses, val_losses, train_accs, val_accs = [], [], [], []  # Stores average loss per epoch\n",
    "\n",
    "    while(1):\n",
    "        epoch_id += 1\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        iteration = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for images, labels in train_loader:\n",
    "            iteration += 1\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "                        \n",
    "            outputs = model(images, depth_level)\n",
    "            loss = nn.CrossEntropyLoss()(outputs, labels)            \n",
    "            loss.backward()           \n",
    "            optimizer.step()                \n",
    "            train_loss += loss.item() \n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == labels).sum().item()            \n",
    "            total += labels.size(0)\n",
    "                              \n",
    "        train_loss /= len(train_loader)\n",
    "        train_losses.append(train_loss) \n",
    "        train_acc = round((correct / total) * 100, 2)\n",
    "        train_accs.append(train_acc)\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = model(images, depth_level)\n",
    "                \n",
    "            loss = nn.CrossEntropyLoss()(outputs, labels)            \n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == labels).sum().item()            \n",
    "            total += labels.size(0)\n",
    "                    \n",
    "        val_loss /= len(val_loader)\n",
    "        val_loss = round(val_loss,4)\n",
    "        val_losses.append(val_loss)\n",
    "        val_acc = round((correct / total) * 100, 2)\n",
    "        val_accs.append(val_acc)\n",
    "        print(f\"Epoch {epoch_id}: Train Loss = {train_loss:.4f}, Depth = {depth_level}, Validation Accuracy = {val_acc:.2f} %\")\n",
    "        \n",
    "        if fine_tune_flag == 0:\n",
    "            val_acc = round((correct / total) * 100, 1)\n",
    "        else:\n",
    "            val_acc = round((correct / total) * 100, 2)\n",
    "        \n",
    "        # Early stopping check\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            print(f\"******* Best Validation Accuracy: {best_val_acc:.2f} %\")\n",
    "            no_improvement = 0\n",
    "            torch.save(model.state_dict(), 'ODN_ResNet18_MNIST.pth')\n",
    "            \n",
    "        else:\n",
    "            no_improvement += 1\n",
    "            if no_improvement % 5 == 0:\n",
    "                print(f\"No Improvement Epochs: {no_improvement}. LR Step Reduced!\")\n",
    "                scheduler.step()\n",
    "                for param_group in optimizer.param_groups:\n",
    "                    print(f\"New Learning Rate: {param_group['lr']}\")\n",
    "        \n",
    "        if (no_improvement >= stopping_epochs) and (best_val_acc <= (target_accuracy - tolerance)) and (fine_tune_flag == 0):\n",
    "            depth_level += 1\n",
    "            best_val_acc = 0.0\n",
    "            no_improvement = 0\n",
    "            if depth_level > max_depth:\n",
    "                depth_level = max_depth     # Clip depth if it surpasses maximum depth\n",
    "            optimizer = SGD(model.parameters(), lr=learning_rate / 2, momentum=0.9, weight_decay=5e-4)\n",
    "            scheduler = lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.6)\n",
    "            print(\"******* Depth Incremented - Previous Depth Converged!\\n\")\n",
    "            model.load_state_dict(torch.load('ODN_ResNet18_MNIST_warmup_model.pth', weights_only=True))    #######################\n",
    "\n",
    "        if fine_tune_flag == 0:\n",
    "            if (no_improvement >= stopping_epochs) or (best_val_acc >= (target_accuracy - tolerance)) or (depth_level == max_depth):\n",
    "                print(f\"\\n*** Model Converged | Depth: {depth_level} | Target Accuracy Reached: {best_val_acc:.2f} %\")\n",
    "                print(\"\\n************************ Fine-Tuning ************************\\n\")\n",
    "                fine_tune_flag = 1\n",
    "            \n",
    "        if (fine_tune_flag == 1) and (no_improvement >= stopping_epochs):\n",
    "            print(f\"\\n\\n*** Model Fine-Tuned | Optimal Depth: {depth_level} | Best Validation Accuracy: {best_val_acc:.2f} %\")\n",
    "            #break\n",
    "            return train_losses, val_losses, train_accs, val_accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Loss: 1.1415 | Accuracy: 67.97 %\n",
      "Epoch: 2 | Loss: 0.5751 | Accuracy: 86.41 %\n",
      "Epoch: 3 | Loss: 0.4630 | Accuracy: 89.62 %\n"
     ]
    }
   ],
   "source": [
    "warm_up_epochs = 3         ###### Set warm-up epochs if warm-up is desired\n",
    "learning_rate = 0.001\n",
    "model = warm_up(model, warm_up_epochs, learning_rate, depths)\n",
    "torch.save(model.state_dict(), 'ODN_ResNet18_MNIST_warmup_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 1.4935, Depth = 1, Validation Accuracy = 84.23 %\n",
      "******* Best Validation Accuracy: 84.20 %\n",
      "Epoch 2: Train Loss = 0.6083, Depth = 1, Validation Accuracy = 84.14 %\n",
      "Epoch 3: Train Loss = 0.3199, Depth = 1, Validation Accuracy = 91.17 %\n",
      "******* Best Validation Accuracy: 91.20 %\n",
      "Epoch 4: Train Loss = 0.2246, Depth = 1, Validation Accuracy = 84.31 %\n",
      "Epoch 5: Train Loss = 0.1863, Depth = 1, Validation Accuracy = 94.63 %\n",
      "******* Best Validation Accuracy: 94.60 %\n",
      "Epoch 6: Train Loss = 0.1643, Depth = 1, Validation Accuracy = 82.67 %\n",
      "Epoch 7: Train Loss = 0.1455, Depth = 1, Validation Accuracy = 94.67 %\n",
      "******* Best Validation Accuracy: 94.70 %\n",
      "Epoch 8: Train Loss = 0.1350, Depth = 1, Validation Accuracy = 94.76 %\n",
      "******* Best Validation Accuracy: 94.80 %\n",
      "Epoch 9: Train Loss = 0.1256, Depth = 1, Validation Accuracy = 93.95 %\n",
      "Epoch 10: Train Loss = 0.1191, Depth = 1, Validation Accuracy = 95.71 %\n",
      "******* Best Validation Accuracy: 95.70 %\n",
      "Epoch 11: Train Loss = 0.1151, Depth = 1, Validation Accuracy = 94.23 %\n",
      "Epoch 12: Train Loss = 0.1090, Depth = 1, Validation Accuracy = 94.74 %\n",
      "Epoch 13: Train Loss = 0.1063, Depth = 1, Validation Accuracy = 84.03 %\n",
      "Epoch 14: Train Loss = 0.1029, Depth = 1, Validation Accuracy = 91.61 %\n",
      "Epoch 15: Train Loss = 0.1002, Depth = 1, Validation Accuracy = 94.80 %\n",
      "No Improvement Epochs: 5. LR Step Reduced!\n",
      "Epoch 16: Train Loss = 0.0884, Depth = 1, Validation Accuracy = 97.31 %\n",
      "******* Best Validation Accuracy: 97.30 %\n",
      "Epoch 17: Train Loss = 0.0878, Depth = 1, Validation Accuracy = 93.23 %\n",
      "Epoch 18: Train Loss = 0.0863, Depth = 1, Validation Accuracy = 96.36 %\n",
      "Epoch 19: Train Loss = 0.0851, Depth = 1, Validation Accuracy = 97.50 %\n",
      "******* Best Validation Accuracy: 97.50 %\n",
      "Epoch 20: Train Loss = 0.0827, Depth = 1, Validation Accuracy = 93.93 %\n",
      "Epoch 21: Train Loss = 0.0827, Depth = 1, Validation Accuracy = 92.88 %\n",
      "Epoch 22: Train Loss = 0.0835, Depth = 1, Validation Accuracy = 80.71 %\n",
      "Epoch 23: Train Loss = 0.0820, Depth = 1, Validation Accuracy = 94.07 %\n",
      "Epoch 24: Train Loss = 0.0801, Depth = 1, Validation Accuracy = 86.13 %\n",
      "No Improvement Epochs: 5. LR Step Reduced!\n",
      "Epoch 25: Train Loss = 0.0745, Depth = 1, Validation Accuracy = 95.13 %\n",
      "Epoch 26: Train Loss = 0.0734, Depth = 1, Validation Accuracy = 97.05 %\n",
      "Epoch 27: Train Loss = 0.0722, Depth = 1, Validation Accuracy = 96.91 %\n",
      "Epoch 28: Train Loss = 0.0723, Depth = 1, Validation Accuracy = 97.08 %\n",
      "Epoch 29: Train Loss = 0.0711, Depth = 1, Validation Accuracy = 96.59 %\n",
      "No Improvement Epochs: 10. LR Step Reduced!\n",
      "Epoch 30: Train Loss = 0.0681, Depth = 1, Validation Accuracy = 97.27 %\n",
      "Epoch 31: Train Loss = 0.0667, Depth = 1, Validation Accuracy = 97.53 %\n",
      "Epoch 32: Train Loss = 0.0658, Depth = 1, Validation Accuracy = 96.39 %\n",
      "Epoch 33: Train Loss = 0.0659, Depth = 1, Validation Accuracy = 95.33 %\n",
      "Epoch 34: Train Loss = 0.0657, Depth = 1, Validation Accuracy = 97.14 %\n",
      "No Improvement Epochs: 15. LR Step Reduced!\n",
      "Epoch 35: Train Loss = 0.0636, Depth = 1, Validation Accuracy = 96.39 %\n",
      "Epoch 36: Train Loss = 0.0626, Depth = 1, Validation Accuracy = 98.11 %\n",
      "******* Best Validation Accuracy: 98.10 %\n",
      "Epoch 37: Train Loss = 0.0620, Depth = 1, Validation Accuracy = 98.07 %\n",
      "Epoch 38: Train Loss = 0.0620, Depth = 1, Validation Accuracy = 98.09 %\n",
      "Epoch 39: Train Loss = 0.0619, Depth = 1, Validation Accuracy = 97.64 %\n",
      "Epoch 40: Train Loss = 0.0625, Depth = 1, Validation Accuracy = 97.66 %\n",
      "Epoch 41: Train Loss = 0.0613, Depth = 1, Validation Accuracy = 97.57 %\n",
      "No Improvement Epochs: 5. LR Step Reduced!\n",
      "Epoch 42: Train Loss = 0.0600, Depth = 1, Validation Accuracy = 97.99 %\n",
      "Epoch 43: Train Loss = 0.0593, Depth = 1, Validation Accuracy = 98.12 %\n",
      "Epoch 44: Train Loss = 0.0604, Depth = 1, Validation Accuracy = 98.37 %\n",
      "******* Best Validation Accuracy: 98.40 %\n",
      "Epoch 45: Train Loss = 0.0591, Depth = 1, Validation Accuracy = 98.23 %\n",
      "Epoch 46: Train Loss = 0.0595, Depth = 1, Validation Accuracy = 98.31 %\n",
      "Epoch 47: Train Loss = 0.0588, Depth = 1, Validation Accuracy = 98.25 %\n",
      "Epoch 48: Train Loss = 0.0591, Depth = 1, Validation Accuracy = 98.22 %\n",
      "Epoch 49: Train Loss = 0.0587, Depth = 1, Validation Accuracy = 97.95 %\n",
      "No Improvement Epochs: 5. LR Step Reduced!\n",
      "Epoch 50: Train Loss = 0.0580, Depth = 1, Validation Accuracy = 98.36 %\n",
      "Epoch 51: Train Loss = 0.0577, Depth = 1, Validation Accuracy = 98.42 %\n",
      "Epoch 52: Train Loss = 0.0577, Depth = 1, Validation Accuracy = 98.15 %\n",
      "Epoch 53: Train Loss = 0.0571, Depth = 1, Validation Accuracy = 98.44 %\n",
      "Epoch 54: Train Loss = 0.0570, Depth = 1, Validation Accuracy = 98.33 %\n",
      "No Improvement Epochs: 10. LR Step Reduced!\n",
      "Epoch 55: Train Loss = 0.0564, Depth = 1, Validation Accuracy = 98.37 %\n",
      "Epoch 56: Train Loss = 0.0564, Depth = 1, Validation Accuracy = 98.35 %\n",
      "Epoch 57: Train Loss = 0.0563, Depth = 1, Validation Accuracy = 98.38 %\n",
      "Epoch 58: Train Loss = 0.0566, Depth = 1, Validation Accuracy = 98.38 %\n",
      "Epoch 59: Train Loss = 0.0559, Depth = 1, Validation Accuracy = 98.48 %\n",
      "******* Best Validation Accuracy: 98.50 %\n",
      "Epoch 60: Train Loss = 0.0560, Depth = 1, Validation Accuracy = 98.40 %\n",
      "Epoch 61: Train Loss = 0.0561, Depth = 1, Validation Accuracy = 98.36 %\n",
      "Epoch 62: Train Loss = 0.0561, Depth = 1, Validation Accuracy = 98.47 %\n",
      "Epoch 63: Train Loss = 0.0562, Depth = 1, Validation Accuracy = 98.41 %\n",
      "Epoch 64: Train Loss = 0.0560, Depth = 1, Validation Accuracy = 98.40 %\n",
      "No Improvement Epochs: 5. LR Step Reduced!\n",
      "Epoch 65: Train Loss = 0.0557, Depth = 1, Validation Accuracy = 98.38 %\n",
      "Epoch 66: Train Loss = 0.0555, Depth = 1, Validation Accuracy = 98.42 %\n",
      "Epoch 67: Train Loss = 0.0559, Depth = 1, Validation Accuracy = 98.36 %\n",
      "Epoch 68: Train Loss = 0.0553, Depth = 1, Validation Accuracy = 98.42 %\n",
      "Epoch 69: Train Loss = 0.0556, Depth = 1, Validation Accuracy = 98.48 %\n",
      "No Improvement Epochs: 10. LR Step Reduced!\n",
      "Epoch 70: Train Loss = 0.0552, Depth = 1, Validation Accuracy = 98.43 %\n",
      "Epoch 71: Train Loss = 0.0549, Depth = 1, Validation Accuracy = 98.43 %\n",
      "Epoch 72: Train Loss = 0.0555, Depth = 1, Validation Accuracy = 98.41 %\n",
      "Epoch 73: Train Loss = 0.0550, Depth = 1, Validation Accuracy = 98.43 %\n",
      "Epoch 74: Train Loss = 0.0544, Depth = 1, Validation Accuracy = 98.45 %\n",
      "No Improvement Epochs: 15. LR Step Reduced!\n",
      "Epoch 75: Train Loss = 0.0548, Depth = 1, Validation Accuracy = 98.47 %\n",
      "Epoch 76: Train Loss = 0.0548, Depth = 1, Validation Accuracy = 98.40 %\n",
      "Epoch 77: Train Loss = 0.0549, Depth = 1, Validation Accuracy = 98.41 %\n",
      "Epoch 78: Train Loss = 0.0546, Depth = 1, Validation Accuracy = 98.39 %\n",
      "Epoch 79: Train Loss = 0.0545, Depth = 1, Validation Accuracy = 98.48 %\n",
      "No Improvement Epochs: 20. LR Step Reduced!\n",
      "Epoch 80: Train Loss = 0.0548, Depth = 1, Validation Accuracy = 98.45 %\n",
      "Epoch 81: Train Loss = 0.0546, Depth = 1, Validation Accuracy = 98.45 %\n",
      "Epoch 82: Train Loss = 0.0550, Depth = 1, Validation Accuracy = 98.45 %\n",
      "\n",
      "******* Best Validation Accuracy: 98.50 %\n",
      "******* Depth Incremented - Previous Depth Converged!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1390410/1510519161.py:93: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('ODN_ResNet18_MNIST_warmup_model.pth'))    #######################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83: Train Loss = 0.4881, Depth = 2, Validation Accuracy = 90.56 %\n",
      "Epoch 84: Train Loss = 0.1913, Depth = 2, Validation Accuracy = 95.81 %\n",
      "Epoch 85: Train Loss = 0.1297, Depth = 2, Validation Accuracy = 96.89 %\n",
      "Epoch 86: Train Loss = 0.1057, Depth = 2, Validation Accuracy = 86.68 %\n",
      "Epoch 87: Train Loss = 0.0891, Depth = 2, Validation Accuracy = 95.32 %\n",
      "No Improvement Epochs: 5. LR Step Reduced!\n",
      "Epoch 88: Train Loss = 0.0715, Depth = 2, Validation Accuracy = 97.43 %\n",
      "Epoch 89: Train Loss = 0.0669, Depth = 2, Validation Accuracy = 97.60 %\n",
      "Epoch 90: Train Loss = 0.0633, Depth = 2, Validation Accuracy = 97.38 %\n",
      "Epoch 91: Train Loss = 0.0601, Depth = 2, Validation Accuracy = 98.23 %\n",
      "Epoch 92: Train Loss = 0.0565, Depth = 2, Validation Accuracy = 98.61 %\n",
      "******* Best Validation Accuracy: 98.60 %\n",
      "Epoch 93: Train Loss = 0.0545, Depth = 2, Validation Accuracy = 96.63 %\n",
      "Epoch 94: Train Loss = 0.0517, Depth = 2, Validation Accuracy = 98.52 %\n",
      "Epoch 95: Train Loss = 0.0491, Depth = 2, Validation Accuracy = 98.79 %\n",
      "******* Best Validation Accuracy: 98.80 %\n",
      "Epoch 96: Train Loss = 0.0480, Depth = 2, Validation Accuracy = 98.50 %\n",
      "Epoch 97: Train Loss = 0.0450, Depth = 2, Validation Accuracy = 98.32 %\n",
      "Epoch 98: Train Loss = 0.0442, Depth = 2, Validation Accuracy = 98.93 %\n",
      "******* Best Validation Accuracy: 98.90 %\n",
      "Epoch 99: Train Loss = 0.0435, Depth = 2, Validation Accuracy = 98.01 %\n",
      "Epoch 100: Train Loss = 0.0410, Depth = 2, Validation Accuracy = 98.17 %\n",
      "Epoch 101: Train Loss = 0.0403, Depth = 2, Validation Accuracy = 98.38 %\n",
      "Epoch 102: Train Loss = 0.0402, Depth = 2, Validation Accuracy = 98.15 %\n",
      "Epoch 103: Train Loss = 0.0383, Depth = 2, Validation Accuracy = 98.20 %\n",
      "No Improvement Epochs: 5. LR Step Reduced!\n",
      "Epoch 104: Train Loss = 0.0341, Depth = 2, Validation Accuracy = 98.55 %\n",
      "Epoch 105: Train Loss = 0.0323, Depth = 2, Validation Accuracy = 99.05 %\n",
      "******* Best Validation Accuracy: 99.10 %\n",
      "\n",
      "*** Model Converged | Depth: 2 | Target Accuracy Reached: 99.10 %\n",
      "\n",
      "************************ Fine-Tuning ************************\n",
      "\n",
      "Epoch 106: Train Loss = 0.0320, Depth = 2, Validation Accuracy = 98.61 %\n",
      "Epoch 107: Train Loss = 0.0309, Depth = 2, Validation Accuracy = 98.40 %\n",
      "Epoch 108: Train Loss = 0.0312, Depth = 2, Validation Accuracy = 98.91 %\n",
      "Epoch 109: Train Loss = 0.0304, Depth = 2, Validation Accuracy = 98.72 %\n",
      "Epoch 110: Train Loss = 0.0299, Depth = 2, Validation Accuracy = 98.58 %\n",
      "No Improvement Epochs: 5. LR Step Reduced!\n",
      "Epoch 111: Train Loss = 0.0276, Depth = 2, Validation Accuracy = 99.21 %\n",
      "******* Best Validation Accuracy: 99.21 %\n",
      "Epoch 112: Train Loss = 0.0268, Depth = 2, Validation Accuracy = 98.40 %\n",
      "Epoch 113: Train Loss = 0.0264, Depth = 2, Validation Accuracy = 99.19 %\n",
      "Epoch 114: Train Loss = 0.0264, Depth = 2, Validation Accuracy = 98.47 %\n",
      "Epoch 115: Train Loss = 0.0258, Depth = 2, Validation Accuracy = 98.64 %\n",
      "Epoch 116: Train Loss = 0.0258, Depth = 2, Validation Accuracy = 99.11 %\n",
      "No Improvement Epochs: 5. LR Step Reduced!\n",
      "Epoch 117: Train Loss = 0.0242, Depth = 2, Validation Accuracy = 99.19 %\n",
      "Epoch 118: Train Loss = 0.0241, Depth = 2, Validation Accuracy = 99.23 %\n",
      "******* Best Validation Accuracy: 99.23 %\n",
      "Epoch 119: Train Loss = 0.0239, Depth = 2, Validation Accuracy = 99.16 %\n",
      "Epoch 120: Train Loss = 0.0238, Depth = 2, Validation Accuracy = 99.09 %\n",
      "Epoch 121: Train Loss = 0.0234, Depth = 2, Validation Accuracy = 99.14 %\n",
      "Epoch 122: Train Loss = 0.0238, Depth = 2, Validation Accuracy = 99.21 %\n",
      "Epoch 123: Train Loss = 0.0233, Depth = 2, Validation Accuracy = 99.17 %\n",
      "No Improvement Epochs: 5. LR Step Reduced!\n",
      "Epoch 124: Train Loss = 0.0222, Depth = 2, Validation Accuracy = 99.30 %\n",
      "******* Best Validation Accuracy: 99.30 %\n",
      "Epoch 125: Train Loss = 0.0223, Depth = 2, Validation Accuracy = 99.28 %\n",
      "Epoch 126: Train Loss = 0.0219, Depth = 2, Validation Accuracy = 99.22 %\n",
      "Epoch 127: Train Loss = 0.0222, Depth = 2, Validation Accuracy = 99.30 %\n",
      "Epoch 128: Train Loss = 0.0219, Depth = 2, Validation Accuracy = 99.26 %\n",
      "Epoch 129: Train Loss = 0.0224, Depth = 2, Validation Accuracy = 99.25 %\n",
      "No Improvement Epochs: 5. LR Step Reduced!\n",
      "Epoch 130: Train Loss = 0.0213, Depth = 2, Validation Accuracy = 99.31 %\n",
      "******* Best Validation Accuracy: 99.31 %\n",
      "Epoch 131: Train Loss = 0.0213, Depth = 2, Validation Accuracy = 99.32 %\n",
      "******* Best Validation Accuracy: 99.32 %\n",
      "Epoch 132: Train Loss = 0.0210, Depth = 2, Validation Accuracy = 99.27 %\n",
      "Epoch 133: Train Loss = 0.0210, Depth = 2, Validation Accuracy = 99.33 %\n",
      "******* Best Validation Accuracy: 99.33 %\n",
      "Epoch 134: Train Loss = 0.0214, Depth = 2, Validation Accuracy = 99.29 %\n",
      "Epoch 135: Train Loss = 0.0210, Depth = 2, Validation Accuracy = 99.28 %\n",
      "Epoch 136: Train Loss = 0.0207, Depth = 2, Validation Accuracy = 99.32 %\n",
      "Epoch 137: Train Loss = 0.0209, Depth = 2, Validation Accuracy = 99.20 %\n",
      "Epoch 138: Train Loss = 0.0206, Depth = 2, Validation Accuracy = 99.30 %\n",
      "No Improvement Epochs: 5. LR Step Reduced!\n",
      "Epoch 139: Train Loss = 0.0204, Depth = 2, Validation Accuracy = 99.35 %\n",
      "******* Best Validation Accuracy: 99.35 %\n",
      "Epoch 140: Train Loss = 0.0202, Depth = 2, Validation Accuracy = 99.26 %\n",
      "Epoch 141: Train Loss = 0.0203, Depth = 2, Validation Accuracy = 99.30 %\n",
      "Epoch 142: Train Loss = 0.0203, Depth = 2, Validation Accuracy = 99.27 %\n",
      "Epoch 143: Train Loss = 0.0201, Depth = 2, Validation Accuracy = 99.34 %\n",
      "Epoch 144: Train Loss = 0.0203, Depth = 2, Validation Accuracy = 99.29 %\n",
      "No Improvement Epochs: 5. LR Step Reduced!\n",
      "Epoch 145: Train Loss = 0.0201, Depth = 2, Validation Accuracy = 99.32 %\n",
      "Epoch 146: Train Loss = 0.0201, Depth = 2, Validation Accuracy = 99.26 %\n",
      "Epoch 147: Train Loss = 0.0199, Depth = 2, Validation Accuracy = 99.33 %\n",
      "Epoch 148: Train Loss = 0.0200, Depth = 2, Validation Accuracy = 99.29 %\n",
      "Epoch 149: Train Loss = 0.0201, Depth = 2, Validation Accuracy = 99.27 %\n",
      "No Improvement Epochs: 10. LR Step Reduced!\n",
      "Epoch 150: Train Loss = 0.0200, Depth = 2, Validation Accuracy = 99.29 %\n",
      "Epoch 151: Train Loss = 0.0200, Depth = 2, Validation Accuracy = 99.30 %\n",
      "Epoch 152: Train Loss = 0.0199, Depth = 2, Validation Accuracy = 99.30 %\n",
      "Epoch 153: Train Loss = 0.0200, Depth = 2, Validation Accuracy = 99.32 %\n",
      "Epoch 154: Train Loss = 0.0200, Depth = 2, Validation Accuracy = 99.34 %\n",
      "No Improvement Epochs: 15. LR Step Reduced!\n",
      "Epoch 155: Train Loss = 0.0196, Depth = 2, Validation Accuracy = 99.30 %\n",
      "Epoch 156: Train Loss = 0.0199, Depth = 2, Validation Accuracy = 99.32 %\n",
      "Epoch 157: Train Loss = 0.0198, Depth = 2, Validation Accuracy = 99.29 %\n",
      "Epoch 158: Train Loss = 0.0196, Depth = 2, Validation Accuracy = 99.31 %\n",
      "Epoch 159: Train Loss = 0.0196, Depth = 2, Validation Accuracy = 99.31 %\n",
      "No Improvement Epochs: 20. LR Step Reduced!\n",
      "Epoch 160: Train Loss = 0.0197, Depth = 2, Validation Accuracy = 99.31 %\n",
      "Epoch 161: Train Loss = 0.0197, Depth = 2, Validation Accuracy = 99.33 %\n",
      "Epoch 162: Train Loss = 0.0196, Depth = 2, Validation Accuracy = 99.28 %\n",
      "\n",
      "\n",
      "*** Model Fine-Tuned | Optimal Depth: 2 | Best Validation Accuracy: 99.35 %\n"
     ]
    }
   ],
   "source": [
    "target_accuracy = 99.5     # Set target accuracy (as per requirement)\n",
    "tolerance = 0.5\n",
    "initial_depth = 1          # Set initial depth (for optimal depth search)\n",
    "max_depth = 8              # Set maximum allowed depth (for optimal depth search)\n",
    "learning_rate = 0.01\n",
    "train_losses, val_losses, train_accs, val_accs = train_till_convergence(model, target_accuracy, tolerance, initial_depth, max_depth, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ResNet18().to(device)\n",
    "model.load_state_dict(torch.load(\"ODN_ResNet18_MNIST.pth\", weights_only=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, depth_level):\n",
    "    model.eval()\n",
    "    requires_grad_(model, False)\n",
    "    test_accs = AverageMeter()\n",
    "    for i, (imgs, labels) in enumerate(test_loader):\n",
    "        imgs, labels = imgs.cuda(), labels.cuda()\n",
    "        logits = model(imgs, depth_level)\n",
    "        test_accs.append((logits.argmax(1) == labels).float().mean().item())\n",
    "    print(f\"Depth Level: {depth_level}  |  Test Accuracy: {test_accs.avg * 100:.2f} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Depth Level: 2  |  Test Accuracy: 99.31 %\n"
     ]
    }
   ],
   "source": [
    "test_model(model, 2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ofa_adv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
