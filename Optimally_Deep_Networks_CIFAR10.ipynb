{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU\n",
      "GPUs Count: 1\n"
     ]
    }
   ],
   "source": [
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "from PIL import Image\n",
    "from torch.optim import SGD, lr_scheduler\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from fvcore.nn import FlopCountAnalysis\n",
    "from utils import requires_grad_, AverageMeter\n",
    "from dataloaders.cifar10 import cifar10_dataloaders\n",
    "from dataloaders.cifar100 import cifar100_dataloaders\n",
    "from dataloaders.svhn import svhn_dataloaders\n",
    "from dataloaders.stl10 import stl10_dataloaders\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('GPU') if str(device) == \"cuda:0\" else print('GPU not Detected - CPU Selected')\n",
    "print(f\"GPUs Count: {torch.cuda.device_count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_train_batch_size = 128\n",
    "DATASET = 'cifar10'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "if DATASET == 'cifar10':\n",
    "    train_loader, val_loader, test_loader = cifar10_dataloaders(train_batch_size=my_train_batch_size, num_workers=2)\n",
    "elif DATASET == 'cifar100':\n",
    "    train_loader, val_loader, test_loader = cifar100_dataloaders(train_batch_size=my_train_batch_size, num_workers=2)\n",
    "elif DATASET == 'svhn':\n",
    "    train_loader, val_loader, test_loader = svhn_dataloaders(train_batch_size=my_train_batch_size, num_workers=2)\n",
    "elif DATASET == 'stl10':\n",
    "    train_loader, val_loader = stl10_dataloaders(train_batch_size=my_train_batch_size, num_workers=2)\n",
    "elif DATASET == 'mnist':\n",
    "    transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "    train_dataset = Subset(datasets.MNIST('./data', train=True, transform=transform, download=True), list(range(50000)))\n",
    "    val_dataset = Subset(datasets.MNIST('./data', train=True, transform=transform, download=True), list(range(50000, 60000)))\n",
    "    test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=my_train_batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=100, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=100, shuffle=False)\n",
    "\n",
    "elif DATASET == 'emnist':\n",
    "    my_split = 'letters'  # options: 'byclass', 'bymerge', 'balanced', 'letters', 'digits', 'mnist'\n",
    "    transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "    train_dataset = datasets.EMNIST(root='./data', split=my_split, train=True, transform=transform, download=True)\n",
    "    test_dataset = datasets.EMNIST(root='./data', split=my_split, train=False, download=True, transform=transform)\n",
    "\n",
    "    train_dataset.targets -= 1    # Shift labels from 1-26 to 0-25\n",
    "    test_dataset.targets -= 1     # Shift labels from 1-26 to 0-25\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=my_train_batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=100, shuffle=False)\n",
    "\n",
    "elif DATASET == 'fashion_mnist':\n",
    "    transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.2860,), (0.3530,))])\n",
    "    train_dataset = Subset(datasets.FashionMNIST('./data', train=True, transform=transform, download=True), list(range(50000)))\n",
    "    val_dataset = Subset(datasets.FashionMNIST('./data', train=True, transform=transform, download=True), list(range(50000, 60000)))\n",
    "    test_dataset = datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=my_train_batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=100, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=100, shuffle=False)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(351, 50, 100)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader), len(val_loader), len(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DATASET == 'mnist' or DATASET == 'emnist' or DATASET == 'fashion_mnist':\n",
    "    image_channels = 1\n",
    "else:\n",
    "    image_channels = 3\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1, downsample=None):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            planes, planes, kernel_size=3, stride=stride, padding=1, bias=False\n",
    "        )\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(\n",
    "            planes, planes * self.expansion, kernel_size=1, bias=False\n",
    "        )\n",
    "        self.bn3 = nn.BatchNorm2d(planes * self.expansion)\n",
    "\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.relu(self.bn2(self.conv2(out)))\n",
    "        out = self.bn3(self.conv3(out))\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, layers, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "\n",
    "        # Initial convolution + maxpool\n",
    "        self.conv1 = nn.Conv2d(image_channels, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        #self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        # ResNet stages\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
    "\n",
    "        self.linear_1 = nn.Linear(4 * 64, num_classes)\n",
    "        self.linear_2 = nn.Linear(4 * 64, num_classes)\n",
    "        self.linear_3 = nn.Linear(4 * 64, num_classes)\n",
    "        self.linear_4 = nn.Linear(4 * 128, num_classes)\n",
    "        self.linear_5 = nn.Linear(4 * 128, num_classes)\n",
    "        self.linear_6 = nn.Linear(4 * 128, num_classes)\n",
    "        self.linear_7 = nn.Linear(4 * 128, num_classes)\n",
    "        self.linear_8 = nn.Linear(4 * 256, num_classes)\n",
    "        self.linear_9 = nn.Linear(4 * 256, num_classes)\n",
    "        self.linear_10 = nn.Linear(4 * 256, num_classes)\n",
    "        self.linear_11 = nn.Linear(4 * 256, num_classes)\n",
    "        self.linear_12 = nn.Linear(4 * 256, num_classes)\n",
    "        self.linear_13 = nn.Linear(4 * 256, num_classes)\n",
    "        self.linear_14 = nn.Linear(4 * 512, num_classes)\n",
    "        self.linear_15 = nn.Linear(4 * 512, num_classes)\n",
    "        self.linear_16 = nn.Linear(4 * 512, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.in_planes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(nn.Conv2d(self.in_planes, planes * block.expansion, kernel_size=1, stride=stride, bias=False), nn.BatchNorm2d(planes * block.expansion))\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.in_planes, planes, stride, downsample))\n",
    "        self.in_planes = planes * block.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(self.in_planes, planes))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x, depth_level=16):\n",
    "        out = self.relu(self.bn1(self.conv1(x)))\n",
    "        #out = self.maxpool(out) ############################### Remove this pooling for MNIST, SVHN, CIFAR-10, CIFAR-100\n",
    "\n",
    "        block_count = 0\n",
    "        for block in self.layer1:\n",
    "            block_count += 1\n",
    "            out = block(out)\n",
    "            if block_count == depth_level:\n",
    "                return self._finalize(out, depth_level)\n",
    "\n",
    "        for block in self.layer2:\n",
    "            block_count += 1\n",
    "            out = block(out)\n",
    "            if block_count == depth_level:\n",
    "                return self._finalize(out, depth_level)\n",
    "\n",
    "        for block in self.layer3:\n",
    "            block_count += 1\n",
    "            out = block(out)\n",
    "            if block_count == depth_level:\n",
    "                return self._finalize(out, depth_level)\n",
    "\n",
    "        for block in self.layer4:\n",
    "            block_count += 1\n",
    "            out = block(out)\n",
    "            if block_count == depth_level:\n",
    "                return self._finalize(out, depth_level)\n",
    "        return self._finalize(out, depth_level)\n",
    "\n",
    "\n",
    "    def _finalize(self, out, depth_level):\n",
    "        out = F.adaptive_avg_pool2d(out, (1, 1))\n",
    "        out = out.view(out.size(0), -1)\n",
    "        \n",
    "        if depth_level == 1:\n",
    "            out = self.linear_1(out)\n",
    "        elif depth_level == 2:\n",
    "            out = self.linear_2(out)\n",
    "        elif depth_level == 3:\n",
    "            out = self.linear_3(out)\n",
    "        elif depth_level == 4:\n",
    "            out = self.linear_4(out)\n",
    "        elif depth_level == 5:\n",
    "            out = self.linear_5(out)\n",
    "        elif depth_level == 6:\n",
    "            out = self.linear_6(out)\n",
    "        elif depth_level == 7:\n",
    "            out = self.linear_7(out)\n",
    "        elif depth_level == 8:\n",
    "            out = self.linear_8(out)\n",
    "        elif depth_level == 9:\n",
    "            out = self.linear_9(out)\n",
    "        elif depth_level == 10:\n",
    "            out = self.linear_10(out)\n",
    "        elif depth_level == 11:\n",
    "            out = self.linear_11(out)\n",
    "        elif depth_level == 12:\n",
    "            out = self.linear_12(out)\n",
    "        elif depth_level == 13:\n",
    "            out = self.linear_13(out)\n",
    "        elif depth_level == 14:\n",
    "            out = self.linear_14(out)\n",
    "        elif depth_level == 15:\n",
    "            out = self.linear_15(out)\n",
    "        elif depth_level == 16:\n",
    "            out = self.linear_16(out)\n",
    "        return out\n",
    "\n",
    "def ResNet50(num_classes=10):\n",
    "    return ResNet(Bottleneck, [3, 4, 6, 3], num_classes=num_classes)\n",
    "\n",
    "model = ResNet50(num_classes=10).to(device)\n",
    "max_depth = 16   # Number of blocks (total depth levels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Training Loop**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "depths = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################### Train Model Till Convergence\n",
    "def warm_up(model, epochs, learning_rate, depths):\n",
    "    optimizer = SGD(model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=5e-4)\n",
    "    scheduler = lr_scheduler.CosineAnnealingLR(optimizer, epochs)\n",
    "    for epoch in range(1, epochs+1):\n",
    "        model.train()\n",
    "        requires_grad_(model, True)\n",
    "        accs, losses = AverageMeter(), AverageMeter()\n",
    "        i = 0\n",
    "        for imgs, labels in train_loader:\n",
    "            i+=1\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            for depth_level in sorted(depths, reverse=True):\n",
    "                logits = model(imgs, depth_level)\n",
    "                loss = torch.mean(F.cross_entropy(logits, labels, reduction='none'))\n",
    "                accs.append((logits.argmax(1) == labels).float().mean().item() * 100)\n",
    "                losses.append(loss.item())\n",
    "                loss.backward()\n",
    "            optimizer.step()\n",
    "        train_str = f\"Epoch: {epoch} | Loss: {losses.avg:.4f} | Accuracy: {accs.avg:.2f} %\"\n",
    "        print(train_str)\n",
    "        scheduler.step()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################### Train Model Till Convergence\n",
    "def train_till_convergence(model, target_accuracy, tolerance = 0.5, initial_depth = 1, max_depth = max_depth, learning_rate = 0.1):\n",
    "    depth_level = initial_depth\n",
    "    fine_tune_flag = 0\n",
    "    stopping_epochs = 23\n",
    "    no_improvement = 0\n",
    "    epoch_id = 0\n",
    "    optimizer = SGD(model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=5e-4)\n",
    "    scheduler = lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.6)\n",
    "\n",
    "    best_val_acc = 0.0\n",
    "    train_losses, val_losses, train_accs, val_accs = [], [], [], []  # Stores average loss per epoch\n",
    "\n",
    "    while(1):\n",
    "        epoch_id += 1\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        iteration = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for images, labels in train_loader:\n",
    "            iteration += 1\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "                        \n",
    "            outputs = model(images, depth_level)\n",
    "            loss = nn.CrossEntropyLoss()(outputs, labels)            \n",
    "            loss.backward()           \n",
    "            optimizer.step()                \n",
    "            train_loss += loss.item() \n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == labels).sum().item()            \n",
    "            total += labels.size(0)\n",
    "                              \n",
    "        train_loss /= len(train_loader)\n",
    "        train_losses.append(train_loss) \n",
    "        train_acc = round((correct / total) * 100, 2)\n",
    "        train_accs.append(train_acc)\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = model(images, depth_level)\n",
    "                \n",
    "            loss = nn.CrossEntropyLoss()(outputs, labels)            \n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == labels).sum().item()            \n",
    "            total += labels.size(0)\n",
    "                    \n",
    "        val_loss /= len(val_loader)\n",
    "        val_loss = round(val_loss,4)\n",
    "        val_losses.append(val_loss)\n",
    "        val_acc = round((correct / total) * 100, 2)\n",
    "        val_accs.append(val_acc)\n",
    "        print(f\"Epoch {epoch_id}: Train Loss = {train_loss:.4f}, Depth = {depth_level}, Validation Accuracy = {val_acc:.2f} %\")\n",
    "        \n",
    "        if fine_tune_flag == 0:\n",
    "            val_acc = round((correct / total) * 100, 1)\n",
    "        else:\n",
    "            val_acc = round((correct / total) * 100, 2)\n",
    "        \n",
    "        # Early stopping check\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            print(f\"******* Best Validation Accuracy: {best_val_acc:.2f} %\")\n",
    "            no_improvement = 0\n",
    "            torch.save(model.state_dict(), 'ODN_ResNet50_CIFAR10.pth')\n",
    "            \n",
    "        else:\n",
    "            no_improvement += 1\n",
    "            if no_improvement % 5 == 0:\n",
    "                print(f\"No Improvement Epochs: {no_improvement}. LR Step Reduced!\")\n",
    "                scheduler.step()\n",
    "                for param_group in optimizer.param_groups:\n",
    "                    print(f\"New Learning Rate: {param_group['lr']}\")\n",
    "        \n",
    "        if (no_improvement >= stopping_epochs) and (best_val_acc <= (target_accuracy - tolerance)) and (fine_tune_flag == 0):\n",
    "            depth_level += 1\n",
    "            best_val_acc = 0.0\n",
    "            no_improvement = 0\n",
    "            if depth_level > max_depth:\n",
    "                depth_level = max_depth     # Clip depth if it surpasses maximum depth\n",
    "            optimizer = SGD(model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=5e-4)\n",
    "            scheduler = lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.6)\n",
    "            print(\"******* Depth Incremented - Previous Depth Converged!\\n\")\n",
    "            model.load_state_dict(torch.load('ODN_ResNet50_CIFAR10_warmup_model.pth', weights_only=True))\n",
    "\n",
    "        if fine_tune_flag == 0:\n",
    "            if (no_improvement >= stopping_epochs) or (best_val_acc >= (target_accuracy - tolerance)) or (depth_level == max_depth):\n",
    "                print(f\"\\n*** Model Converged | Depth: {depth_level} | Target Accuracy Reached: {best_val_acc:.2f} %\")\n",
    "                print(\"\\n************************ Fine-Tuning ************************\\n\")\n",
    "                fine_tune_flag = 1\n",
    "            \n",
    "        if (fine_tune_flag == 1) and (no_improvement >= stopping_epochs):\n",
    "            print(f\"\\n\\n*** Model Fine-Tuned | Optimal Depth: {depth_level} | Best Validation Accuracy: {best_val_acc:.2f} %\")\n",
    "            #break\n",
    "            return train_losses, val_losses, train_accs, val_accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warm_up_epochs = 0         ###### Set warm-up epochs if warm-up is desired\n",
    "learning_rate = 0.01\n",
    "model = warm_up(model, warm_up_epochs, learning_rate, depths)\n",
    "torch.save(model.state_dict(), 'ODN_ResNet50_CIFAR10_warmup_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_accuracy = 94.5     # Set target accuracy (as per requirement)\n",
    "tolerance = 0.5\n",
    "initial_depth = 1          # Set initial depth (for optimal depth search)\n",
    "max_depth = 16             # Set maximum allowed depth (for optimal depth search)\n",
    "learning_rate = 0.01\n",
    "train_losses, val_losses, train_accs, val_accs = train_till_convergence(model, target_accuracy, tolerance, initial_depth, max_depth, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ResNet50().to(device)\n",
    "model.load_state_dict(torch.load(\"ODN_ResNet50_CIFAR10.pth\", weights_only=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, depth_level):\n",
    "    model.eval()\n",
    "    requires_grad_(model, False)\n",
    "    test_accs = AverageMeter()\n",
    "    for i, (imgs, labels) in enumerate(test_loader):\n",
    "        imgs, labels = imgs.cuda(), labels.cuda()\n",
    "        logits = model(imgs, depth_level)\n",
    "        test_accs.append((logits.argmax(1) == labels).float().mean().item())\n",
    "    print(f\"Depth Level: {depth_level}  |  Test Accuracy: {test_accs.avg * 100:.2f} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Depth Level: 11  |  Test Accuracy: 93.35 %\n"
     ]
    }
   ],
   "source": [
    "test_model(model, 11)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ofa_adv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
