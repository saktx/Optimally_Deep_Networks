{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU\n",
      "GPUs Count: 1\n"
     ]
    }
   ],
   "source": [
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "from PIL import Image\n",
    "from torch.optim import SGD, lr_scheduler\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from fvcore.nn import FlopCountAnalysis\n",
    "from utils import requires_grad_, AverageMeter\n",
    "from dataloaders.cifar10 import cifar10_dataloaders\n",
    "from dataloaders.cifar100 import cifar100_dataloaders\n",
    "from dataloaders.svhn import svhn_dataloaders\n",
    "from dataloaders.stl10 import stl10_dataloaders\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('GPU') if str(device) == \"cuda:0\" else print('GPU not Detected - CPU Selected')\n",
    "print(f\"GPUs Count: {torch.cuda.device_count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_train_batch_size = 128\n",
    "DATASET = 'emnist'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DATASET == 'cifar10':\n",
    "    train_loader, val_loader, test_loader = cifar10_dataloaders(train_batch_size=my_train_batch_size, num_workers=2)\n",
    "elif DATASET == 'cifar100':\n",
    "    train_loader, val_loader, test_loader = cifar100_dataloaders(train_batch_size=my_train_batch_size, num_workers=2)\n",
    "elif DATASET == 'svhn':\n",
    "    train_loader, val_loader, test_loader = svhn_dataloaders(train_batch_size=my_train_batch_size, num_workers=2)\n",
    "elif DATASET == 'stl10':\n",
    "    train_loader, val_loader = stl10_dataloaders(train_batch_size=my_train_batch_size, num_workers=2)\n",
    "elif DATASET == 'mnist':\n",
    "    transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "    train_dataset = Subset(datasets.MNIST('./data', train=True, transform=transform, download=True), list(range(50000)))\n",
    "    val_dataset = Subset(datasets.MNIST('./data', train=True, transform=transform, download=True), list(range(50000, 60000)))\n",
    "    test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=my_train_batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=100, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=100, shuffle=False)\n",
    "\n",
    "elif DATASET == 'emnist':\n",
    "    my_split = 'letters'  # options: 'byclass', 'bymerge', 'balanced', 'letters', 'digits', 'mnist'\n",
    "    transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "    train_dataset = datasets.EMNIST(root='./data', split=my_split, train=True, transform=transform, download=True)\n",
    "    test_dataset = datasets.EMNIST(root='./data', split=my_split, train=False, download=True, transform=transform)\n",
    "\n",
    "    train_dataset.targets -= 1    # Shift labels from 1-26 to 0-25\n",
    "    test_dataset.targets -= 1     # Shift labels from 1-26 to 0-25\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=my_train_batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=100, shuffle=False)\n",
    "\n",
    "elif DATASET == 'fashion_mnist':\n",
    "    transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.2860,), (0.3530,))])\n",
    "    train_dataset = Subset(datasets.FashionMNIST('./data', train=True, transform=transform, download=True), list(range(50000)))\n",
    "    val_dataset = Subset(datasets.FashionMNIST('./data', train=True, transform=transform, download=True), list(range(50000, 60000)))\n",
    "    test_dataset = datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=my_train_batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=100, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=100, shuffle=False)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(975, 208)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader), len(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DATASET == 'mnist' or DATASET == 'emnist' or DATASET == 'fashion_mnist':\n",
    "    image_channels = 1\n",
    "else:\n",
    "    image_channels = 3\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self, in_planes, mid_planes, out_planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, mid_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(mid_planes)\n",
    "        self.conv2 = nn.Conv2d(mid_planes, out_planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_planes)\n",
    "\n",
    "        if stride != 1 or in_planes != out_planes:\n",
    "            self.mismatch = True\n",
    "            self.conv_sc = nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
    "            self.bn_sc = nn.BatchNorm2d(out_planes)\n",
    "        else:\n",
    "            self.mismatch = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "\n",
    "        if self.mismatch:\n",
    "            out = out + self.bn_sc(self.conv_sc(x))\n",
    "        else:\n",
    "            out = out + x\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "class ResNet18(nn.Module):\n",
    "    def __init__(self, input_channels = image_channels, num_classes = 26):\n",
    "        super(ResNet18, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(input_channels, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        self.blocks = nn.ModuleList([\n",
    "            BasicBlock(64, 64, 64, stride=1),\n",
    "            BasicBlock(64, 64, 64, stride=1),\n",
    "\n",
    "            BasicBlock(64, 128, 128, stride=2),\n",
    "            BasicBlock(128, 128, 128, stride=1),\n",
    "\n",
    "            BasicBlock(128, 256, 256, stride=2),\n",
    "            BasicBlock(256, 256, 256, stride=1),\n",
    "\n",
    "            BasicBlock(256, 512, 512, stride=2),\n",
    "            BasicBlock(512, 512, 512, stride=1),\n",
    "            ])\n",
    "\n",
    "        self.linear_1 = nn.Linear(64, num_classes)\n",
    "        self.linear_2 = nn.Linear(64, num_classes)\n",
    "        self.linear_3 = nn.Linear(128, num_classes)\n",
    "        self.linear_4 = nn.Linear(128, num_classes)\n",
    "        self.linear_5 = nn.Linear(256, num_classes)\n",
    "        self.linear_6 = nn.Linear(256, num_classes)\n",
    "        self.linear_7 = nn.Linear(512, num_classes)\n",
    "        self.linear_8 = nn.Linear(512, num_classes)\n",
    "        \n",
    "    def forward(self, x, depth_level=8):      \n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        i = 0\n",
    "        for block in self.blocks:\n",
    "            i=i+1\n",
    "            out = block(out)\n",
    "            if i == depth_level:\n",
    "                break\n",
    "                        \n",
    "        out = F.adaptive_avg_pool2d(out, (1, 1))\n",
    "        out = out.view(out.size(0), -1)\n",
    "        \n",
    "        if depth_level == 1:\n",
    "            out = self.linear_1(out)\n",
    "        elif depth_level == 2:\n",
    "            out = self.linear_2(out)\n",
    "        elif depth_level == 3:\n",
    "            out = self.linear_3(out)\n",
    "        elif depth_level == 4:\n",
    "            out = self.linear_4(out)\n",
    "        elif depth_level == 5:\n",
    "            out = self.linear_5(out)\n",
    "        elif depth_level == 6:\n",
    "            out = self.linear_6(out)\n",
    "        elif depth_level == 7:\n",
    "            out = self.linear_7(out)\n",
    "        elif depth_level == 8:\n",
    "            out = self.linear_8(out)\n",
    "        return out\n",
    "\n",
    "model = ResNet18().to(device)\n",
    "max_depth = 8   # Number of blocks (total depth levels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Training Loop**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "depths = [1,2,3,4,5,6,7,8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################### Train Model Till Convergence\n",
    "def warm_up(model, epochs, learning_rate, depths):\n",
    "    optimizer = SGD(model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=5e-4)\n",
    "    scheduler = lr_scheduler.CosineAnnealingLR(optimizer, epochs)\n",
    "    for epoch in range(1, epochs+1):\n",
    "        model.train()\n",
    "        requires_grad_(model, True)\n",
    "        accs, losses = AverageMeter(), AverageMeter()\n",
    "        i = 0\n",
    "        for imgs, labels in train_loader:\n",
    "            i+=1\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            for depth_level in sorted(depths, reverse=True):\n",
    "                logits = model(imgs, depth_level)\n",
    "                loss = torch.mean(F.cross_entropy(logits, labels, reduction='none'))\n",
    "                accs.append((logits.argmax(1) == labels).float().mean().item() * 100)\n",
    "                losses.append(loss.item())\n",
    "                loss.backward()\n",
    "            optimizer.step()\n",
    "        train_str = f\"Epoch: {epoch} | Loss: {losses.avg:.4f} | Accuracy: {accs.avg:.2f} %\"\n",
    "        print(train_str)\n",
    "        scheduler.step()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################### Train Model Till Convergence\n",
    "def train_till_convergence(model, target_accuracy, tolerance = 0.5, initial_depth = 1, max_depth = max_depth, learning_rate = 0.1):\n",
    "    depth_level = initial_depth\n",
    "    fine_tune_flag = 0\n",
    "    stopping_epochs = 23\n",
    "    no_improvement = 0\n",
    "    epoch_id = 0\n",
    "    optimizer = SGD(model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=5e-4)\n",
    "    scheduler = lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.6)\n",
    "\n",
    "    best_val_acc = 0.0\n",
    "    train_losses, val_losses, train_accs, val_accs = [], [], [], []  # Stores average loss per epoch\n",
    "\n",
    "    while(1):\n",
    "        epoch_id += 1\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        iteration = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for images, labels in train_loader:\n",
    "            iteration += 1\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "                        \n",
    "            outputs = model(images, depth_level)\n",
    "            loss = nn.CrossEntropyLoss()(outputs, labels)            \n",
    "            loss.backward()           \n",
    "            optimizer.step()                \n",
    "            train_loss += loss.item() \n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == labels).sum().item()            \n",
    "            total += labels.size(0)\n",
    "                              \n",
    "        train_loss /= len(train_loader)\n",
    "        train_losses.append(train_loss) \n",
    "        train_acc = round((correct / total) * 100, 2)\n",
    "        train_accs.append(train_acc)\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = model(images, depth_level)\n",
    "                \n",
    "            loss = nn.CrossEntropyLoss()(outputs, labels)            \n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == labels).sum().item()            \n",
    "            total += labels.size(0)\n",
    "                    \n",
    "        val_loss /= len(val_loader)\n",
    "        val_loss = round(val_loss,4)\n",
    "        val_losses.append(val_loss)\n",
    "        val_acc = round((correct / total) * 100, 2)\n",
    "        val_accs.append(val_acc)\n",
    "        print(f\"Epoch {epoch_id}: Train Loss = {train_loss:.4f}, Depth = {depth_level}, Validation Accuracy = {val_acc:.2f} %\")\n",
    "        \n",
    "        if fine_tune_flag == 0:\n",
    "            val_acc = round((correct / total) * 100, 1)\n",
    "        else:\n",
    "            val_acc = round((correct / total) * 100, 2)\n",
    "        \n",
    "        # Early stopping check\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            print(f\"******* Best Validation Accuracy: {best_val_acc:.2f} %\")\n",
    "            no_improvement = 0\n",
    "            torch.save(model.state_dict(), 'ODN_ResNet18_ExtendedMNIST.pth')\n",
    "            \n",
    "        else:\n",
    "            no_improvement += 1\n",
    "            if no_improvement % 5 == 0:\n",
    "                print(f\"No Improvement Epochs: {no_improvement}. LR Step Reduced!\")\n",
    "                scheduler.step()\n",
    "                for param_group in optimizer.param_groups:\n",
    "                    print(f\"New Learning Rate: {param_group['lr']}\")\n",
    "        \n",
    "        if (no_improvement >= stopping_epochs) and (best_val_acc <= (target_accuracy - tolerance)) and (fine_tune_flag == 0):\n",
    "            depth_level += 1\n",
    "            best_val_acc = 0.0\n",
    "            no_improvement = 0\n",
    "            if depth_level > max_depth:\n",
    "                depth_level = max_depth     # Clip depth if it surpasses maximum depth\n",
    "            optimizer = SGD(model.parameters(), lr=learning_rate / 2, momentum=0.9, weight_decay=5e-4)\n",
    "            scheduler = lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.6)\n",
    "            print(\"******* Depth Incremented - Previous Depth Converged!\\n\")\n",
    "            model.load_state_dict(torch.load('ODN_ResNet18_ExtendedMNIST_warmup_model.pth', weights_only=True))    #######################\n",
    "\n",
    "        if fine_tune_flag == 0:\n",
    "            if (no_improvement >= stopping_epochs) or (best_val_acc >= (target_accuracy - tolerance)) or (depth_level == max_depth):\n",
    "                print(f\"\\n*** Model Converged | Depth: {depth_level} | Target Accuracy Reached: {best_val_acc:.2f} %\")\n",
    "                print(\"\\n************************ Fine-Tuning ************************\\n\")\n",
    "                fine_tune_flag = 1\n",
    "            \n",
    "        if (fine_tune_flag == 1) and (no_improvement >= stopping_epochs):\n",
    "            print(f\"\\n\\n*** Model Fine-Tuned | Optimal Depth: {depth_level} | Best Validation Accuracy: {best_val_acc:.2f} %\")\n",
    "            #break\n",
    "            return train_losses, val_losses, train_accs, val_accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warm_up_epochs = 3         ###### Set warm-up epochs if warm-up is desired\n",
    "learning_rate = 0.001\n",
    "model = warm_up(model, warm_up_epochs, learning_rate, depths)\n",
    "torch.save(model.state_dict(), 'ODN_ResNet18_ExtendedMNIST_warmup_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_accuracy = 95.0    # Set target accuracy (as per requirement)\n",
    "tolerance = 0.5\n",
    "initial_depth = 1          # Set initial depth (for optimal depth search)\n",
    "max_depth = 8              # Set maximum allowed depth (for optimal depth search)\n",
    "learning_rate = 0.01\n",
    "train_losses, val_losses, train_accs, val_accs = train_till_convergence(model, target_accuracy, tolerance, initial_depth, max_depth, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ResNet18().to(device)\n",
    "model.load_state_dict(torch.load(\"ODN_ResNet18_ExtendedMNIST.pth\", weights_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, depth_level):\n",
    "    model.eval()\n",
    "    requires_grad_(model, False)\n",
    "    test_accs = AverageMeter()\n",
    "    for i, (imgs, labels) in enumerate(test_loader):\n",
    "        imgs, labels = imgs.cuda(), labels.cuda()\n",
    "        logits = model(imgs, depth_level)\n",
    "        test_accs.append((logits.argmax(1) == labels).float().mean().item())\n",
    "    print(f\"Depth Level: {depth_level}  |  Test Accuracy: {test_accs.avg * 100:.2f} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Depth Level: 3  |  Test Accuracy: 95.04 %\n"
     ]
    }
   ],
   "source": [
    "test_model(model, 3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ofa_adv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
